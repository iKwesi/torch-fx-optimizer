{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Graph Optimizer Essentials\n",
    "\n",
    "**Goal**: Learn ONLY what you need to build a graph optimizer using torch.fx\n",
    "\n",
    "**Time**: ~30 minutes\n",
    "\n",
    "**Environment**: M2 Mac with MPS support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx\n",
    "from typing import Dict\n",
    "\n",
    "# Set up MPS device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. COMPUTATIONAL GRAPHS (5 min)\n",
    "\n",
    "### What is a Computation Graph?\n",
    "\n",
    "A computation graph is a directed acyclic graph (DAG) where:\n",
    "- **Nodes** = operations (add, multiply, conv2d, relu, etc.)\n",
    "- **Edges** = data flowing between operations (tensors)\n",
    "- **Graph** = complete sequence of operations from inputs to outputs\n",
    "\n",
    "PyTorch builds this graph **dynamically** during the forward pass. When you call `z = x + y`, PyTorch creates a node representing addition and connects it to nodes for x and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How PyTorch Builds Graphs During Forward Pass\n",
    "\n",
    "When `requires_grad=True`, PyTorch builds an **autograd graph** to track operations for backpropagation. Each tensor remembers the operation that created it via `.grad_fn`. This is NOT the same as torch.fx's symbolic graph (we'll get to that), but it's the foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.grad_fn: <MulBackward0 object at 0x10adc8ca0>\n",
      "w.grad_fn: <AddBackward0 object at 0x1127d6cb0>\n"
     ]
    }
   ],
   "source": [
    "# Simple example: autograd graph\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "z = x * y  # multiplication node\n",
    "w = z + x  # addition node\n",
    "\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")  # Shows the operation that created z\n",
    "print(f\"w.grad_fn: {w.grad_fn}\")  # Shows the operation that created w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example: 2-Layer MLP Graph\n",
    "\n",
    "Let's build a tiny 2-layer MLP and understand what graph PyTorch conceptually builds. The graph has: Input → Linear1 → ReLU → Linear2 → Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 10])\n",
      "Output shape: torch.Size([1, 5])\n",
      "\n",
      "Conceptual graph: Input(1,10) → Linear(10→20) → ReLU → Linear(20→5) → Output(1,5)\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Graph: x → fc1 → relu → fc2 → out\n",
    "        x = self.fc1(x)      # Node 1: Linear transformation\n",
    "        x = torch.relu(x)    # Node 2: ReLU activation\n",
    "        x = self.fc2(x)      # Node 3: Linear transformation\n",
    "        return x\n",
    "\n",
    "model = SimpleMLP().to(device)\n",
    "input_tensor = torch.randn(1, 10, device=device)\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nConceptual graph: Input(1,10) → Linear(10→20) → ReLU → Linear(20→5) → Output(1,5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create a Tiny Model and Explain Its Graph\n",
    "\n",
    "**Task**: Create a model with Conv2d → BatchNorm2d → ReLU → MaxPool2d. Describe the computation graph in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:          torch.Size([1, 3, 32, 32])\n",
      "After Conv1:    torch.Size([1, 16, 32, 32])\n",
      "After ReLU:     torch.Size([1, 16, 32, 32])\n",
      "After Pool1:    torch.Size([1, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "class TinyConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        print(f\"Input:          {x.shape}\") \n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        print(f\"After Conv1:    {x.shape}\")  \n",
    "        \n",
    "        x = self.relu1(x)\n",
    "        print(f\"After ReLU:     {x.shape}\")  \n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        print(f\"After Pool1:    {x.shape}\")  \n",
    "        \n",
    "        return x\n",
    "\n",
    "conv_model = TinyConvNet().to(device)\n",
    "test_input = torch.randn(1, 3, 32, 32, device=device)\n",
    "test_output = conv_model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 32, 32])\n",
      "Output shape: torch.Size([1, 16, 16, 16])\n",
      "\n",
      "Graph: Input(1,3,32,32) → Conv2d → BatchNorm2d → ReLU → MaxPool2d → Output(1,16,16,16)\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "class TinyConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Graph: x → conv2d → batch_norm → relu → max_pool → out\n",
    "        x = self.conv(x)      # Node 1: Convolution\n",
    "        x = self.bn(x)        # Node 2: Batch normalization\n",
    "        x = torch.relu(x)     # Node 3: ReLU activation\n",
    "        x = self.pool(x)      # Node 4: Max pooling\n",
    "        return x\n",
    "\n",
    "conv_model = TinyConvNet().to(device)\n",
    "test_input = torch.randn(1, 3, 32, 32, device=device)\n",
    "test_output = conv_model(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"\\nGraph: Input(1,3,32,32) → Conv2d → BatchNorm2d → ReLU → MaxPool2d → Output(1,16,16,16)\")\n",
    "\n",
    "# Verify output shape\n",
    "assert test_output.shape == torch.Size([1, 16, 16, 16]), \"Output shape mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. TORCH.FX BASICS (10 min)\n",
    "\n",
    "### What is Symbolic Tracing?\n",
    "\n",
    "**Symbolic tracing** captures the operations of your model into an explicit, inspectable graph WITHOUT actually executing them with real data. torch.fx runs your model with \"symbolic\" inputs (Proxy objects) and records every operation into a `GraphModule`. This gives you a **static representation** of the computation graph you can modify before execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How torch.fx Differs from Regular PyTorch\n",
    "\n",
    "- **Regular PyTorch**: Dynamic execution, operations run immediately, graph exists only for autograd\n",
    "- **torch.fx**: Symbolic execution, captures operations into a graph IR, lets you inspect/modify before running\n",
    "- **Key benefit**: You can analyze and transform the graph structure programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Traced Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%fc1,), kwargs = {})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%relu,), kwargs = {})\n",
      "    return fc2\n",
      "\n",
      "=== Generated Python Code ===\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    fc1 = self.fc1(x);  x = None\n",
      "    relu = torch.relu(fc1);  fc1 = None\n",
      "    fc2 = self.fc2(relu);  relu = None\n",
      "    return fc2\n",
      "    \n",
      "\n",
      "✓ Traced model produces identical output\n"
     ]
    }
   ],
   "source": [
    "# Example: Trace the SimpleMLP\n",
    "mlp = SimpleMLP().to(device)\n",
    "\n",
    "# Symbolic tracing - this doesn't run the model, just records operations\n",
    "traced_mlp = fx.symbolic_trace(mlp)\n",
    "\n",
    "print(\"=== Traced Graph ===\")\n",
    "print(traced_mlp.graph)\n",
    "\n",
    "print(\"\\n=== Generated Python Code ===\")\n",
    "print(traced_mlp.code)\n",
    "\n",
    "# Verify it still works\n",
    "test_input = torch.randn(1, 10, device=device)\n",
    "original_output = mlp(test_input)\n",
    "traced_output = traced_mlp(test_input)\n",
    "\n",
    "assert torch.allclose(original_output, traced_output), \"Traced model output differs!\"\n",
    "print(\"\\n✓ Traced model produces identical output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Trace a CNN and Inspect Its Nodes\n",
    "\n",
    "**Task**: Create a simple CNN with Conv2d → ReLU → Conv2d → ReLU. Trace it and print:\n",
    "1. The complete graph\n",
    "2. The number of nodes\n",
    "3. The generated Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Traced Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%conv1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.relu](args = (%conv2,), kwargs = {})\n",
      "    return relu_1\n",
      "\n",
      "=== Generated Python Code ===\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    relu = torch.relu(conv1);  conv1 = None\n",
      "    conv2 = self.conv2(relu);  relu = None\n",
      "    relu_1 = torch.relu(conv2);  conv2 = None\n",
      "    return relu_1\n",
      "    \n",
      "\n",
      "=== Number of Nodes: 6 ===\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        x = self.conv1(x) \n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x) \n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "my_cnn = SimpleCNN().to(device)  \n",
    "trace_cnn = fx.symbolic_trace(my_cnn) \n",
    "\n",
    "print(\"=== Traced Graph ===\")\n",
    "print(trace_cnn.graph)\n",
    "\n",
    "print(\"\\n=== Generated Python Code ===\")\n",
    "print(trace_cnn.code)\n",
    "\n",
    "print(f\"\\n=== Number of Nodes: {len(trace_cnn.graph.nodes)} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%conv1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.relu](args = (%conv2,), kwargs = {})\n",
      "    return relu_1\n",
      "\n",
      "=== Number of Nodes: 6 ===\n",
      "\n",
      "=== Generated Code ===\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    relu = torch.relu(conv1);  conv1 = None\n",
      "    conv2 = self.conv2(relu);  relu = None\n",
      "    relu_1 = torch.relu(conv2);  conv2 = None\n",
      "    return relu_1\n",
      "    \n",
      "\n",
      "✓ Traced CNN works correctly\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "cnn = SimpleCNN().to(device)\n",
    "traced_cnn = fx.symbolic_trace(cnn)\n",
    "\n",
    "print(\"=== Graph ===\")\n",
    "print(traced_cnn.graph)\n",
    "\n",
    "print(f\"\\n=== Number of Nodes: {len(traced_cnn.graph.nodes)} ===\")\n",
    "\n",
    "print(\"\\n=== Generated Code ===\")\n",
    "print(traced_cnn.code)\n",
    "\n",
    "# Verify\n",
    "test_input = torch.randn(1, 3, 32, 32, device=device)\n",
    "original = cnn(test_input)\n",
    "traced = traced_cnn(test_input)\n",
    "assert torch.allclose(original, traced), \"Outputs differ!\"\n",
    "print(\"\\n✓ Traced CNN works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. GRAPH INSPECTION (10 min)\n",
    "\n",
    "### How to Iterate Over Graph Nodes\n",
    "\n",
    "Every `GraphModule` has a `.graph` attribute containing nodes. You iterate with `graph.nodes`. Each node has properties: `op` (operation type), `target` (what's being called), `args` (inputs), and `name` (unique identifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Node Iteration ===\n",
      "Node: x               | Op: placeholder     | Target: x\n",
      "Node: fc1             | Op: call_module     | Target: fc1\n",
      "Node: relu            | Op: call_function   | Target: <built-in method relu of type object at 0x113f135f8>\n",
      "Node: fc2             | Op: call_module     | Target: fc2\n",
      "Node: output          | Op: output          | Target: output\n"
     ]
    }
   ],
   "source": [
    "# Iterate over nodes in traced MLP\n",
    "print(\"=== Node Iteration ===\")\n",
    "for node in traced_mlp.graph.nodes:\n",
    "    print(f\"Node: {node.name:15} | Op: {node.op:15} | Target: {node.target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Node Types, Inputs, and Outputs\n",
    "\n",
    "**Node types (op field)**:\n",
    "- `placeholder`: Input to the graph\n",
    "- `call_module`: Calls a nn.Module (e.g., nn.Linear)\n",
    "- `call_function`: Calls a function (e.g., torch.relu)\n",
    "- `call_method`: Calls a tensor method (e.g., .view())\n",
    "- `output`: Output of the graph\n",
    "\n",
    "**Node connections**:\n",
    "- `node.args`: Tuple of input nodes\n",
    "- `node.users`: Dict of nodes that use this node's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Detailed Node Information ===\n",
      "\n",
      "Node: x\n",
      "  Op type: placeholder\n",
      "  Target: x\n",
      "  Args: ()\n",
      "  Users: [fc1]\n",
      "\n",
      "Node: fc1\n",
      "  Op type: call_module\n",
      "  Target: fc1\n",
      "  Args: (x,)\n",
      "  Users: [relu]\n",
      "\n",
      "Node: relu\n",
      "  Op type: call_function\n",
      "  Target: <built-in method relu of type object at 0x113f135f8>\n",
      "  Args: (fc1,)\n",
      "  Users: [fc2]\n",
      "\n",
      "Node: fc2\n",
      "  Op type: call_module\n",
      "  Target: fc2\n",
      "  Args: (relu,)\n",
      "  Users: [output]\n",
      "\n",
      "Node: output\n",
      "  Op type: output\n",
      "  Target: output\n",
      "  Args: (fc2,)\n",
      "  Users: []\n"
     ]
    }
   ],
   "source": [
    "# Detailed node inspection\n",
    "print(\"=== Detailed Node Information ===\")\n",
    "for node in traced_mlp.graph.nodes:\n",
    "    print(f\"\\nNode: {node.name}\")\n",
    "    print(f\"  Op type: {node.op}\")\n",
    "    print(f\"  Target: {node.target}\")\n",
    "    print(f\"  Args: {node.args}\")\n",
    "    print(f\"  Users: {list(node.users.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Graph Structure\n",
    "\n",
    "The graph is a linked list of nodes in execution order. You can traverse forward (via `.users`) or backward (via `.args`). This structure lets you analyze data flow and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finding ReLU Nodes ===\n",
      "Found ReLU: relu\n",
      "  Input from: conv1\n",
      "  Used by: ['conv2']\n",
      "Found ReLU: relu_1\n",
      "  Input from: conv2\n",
      "  Used by: ['output']\n",
      "\n",
      "Total ReLU nodes: 2\n"
     ]
    }
   ],
   "source": [
    "# Find all ReLU operations\n",
    "print(\"=== Finding ReLU Nodes ===\")\n",
    "relu_nodes = []\n",
    "for node in traced_cnn.graph.nodes:\n",
    "    if node.op == 'call_function' and node.target == torch.relu:\n",
    "        relu_nodes.append(node)\n",
    "        print(f\"Found ReLU: {node.name}\")\n",
    "        print(f\"  Input from: {node.args[0].name if node.args else 'None'}\")\n",
    "        print(f\"  Used by: {[user.name for user in node.users.keys()]}\")\n",
    "\n",
    "print(f\"\\nTotal ReLU nodes: {len(relu_nodes)}\")\n",
    "assert len(relu_nodes) == 2, \"Should have 2 ReLU nodes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Find All 'conv2d' Operations in a Traced ResNet Block\n",
    "\n",
    "**Task**: Create a basic ResNet block (two conv layers with a skip connection). Trace it and find all Conv2d operations. Print their names and input/output connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Traced Graph ===\n",
      "graph():\n",
      "    %x : [num_users=2] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%conv1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%conv2, %x), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.relu](args = (%add,), kwargs = {})\n",
      "    return relu_1\n",
      "\n",
      "=== Generated Python Code ===\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x)\n",
      "    relu = torch.relu(conv1);  conv1 = None\n",
      "    conv2 = self.conv2(relu);  relu = None\n",
      "    add = conv2 + x;  conv2 = x = None\n",
      "    relu_1 = torch.relu(add);  add = None\n",
      "    return relu_1\n",
      "    \n",
      "\n",
      "=== Find Conv2d Operations ===\n",
      "Conv2d module: conv1\n",
      "  node.name: conv1\n",
      "  inputs from: ['x']\n",
      "  outputs to : ['relu']\n",
      "Conv2d module: conv2\n",
      "  node.name: conv2\n",
      "  inputs from: ['relu']\n",
      "  outputs to : ['add']\n",
      "\n",
      "✓ Traced ResNet block works correctly\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        output = self.conv1(x)\n",
    "        output = torch.relu(output)\n",
    "        output = self.conv2(output)\n",
    "        output = output + x\n",
    "        output = torch.relu(output)\n",
    "        return output\n",
    "        \n",
    "resnet = ResNetBlock(64).to(device)\n",
    "trace_resnet = fx.symbolic_trace(resnet)\n",
    "\n",
    "print(\"=== Traced Graph ===\")\n",
    "print(trace_resnet.graph)\n",
    "\n",
    "print(\"\\n=== Generated Python Code ===\")\n",
    "print(trace_resnet.code)\n",
    "\n",
    "print(f\"\\n=== Find Conv2d Operations ===\")\n",
    "\n",
    "# 2) Define precisely what \"Conv2d operation\" means (no string guessing)\n",
    "def is_conv2d_call(node: fx.Node, gm: fx.GraphModule) -> bool:\n",
    "    if node.op != \"call_module\":\n",
    "        return False\n",
    "    submod = gm.get_submodule(node.target)  # <- API, not string parsing\n",
    "    return isinstance(submod, nn.Conv2d)\n",
    "\n",
    "# 3) Walk the graph and print connections using FX graph APIs\n",
    "for node in trace_resnet.graph.nodes:\n",
    "    if not is_conv2d_call(node, trace_resnet):\n",
    "        continue\n",
    "\n",
    "    conv_name = node.target  # e.g., \"conv1\", \"conv2\"\n",
    "\n",
    "    # upstream connections (nodes that feed into this conv)\n",
    "    in_nodes = list(node.all_input_nodes)   # Nodes only (cleaner than .args)\n",
    "    in_names = [n.name for n in in_nodes]\n",
    "\n",
    "    # downstream connections (nodes that consume this conv's output)\n",
    "    out_nodes = list(node.users.keys())\n",
    "    out_names = [n.name for n in out_nodes]\n",
    "\n",
    "    print(f\"Conv2d module: {conv_name}\")\n",
    "    print(f\"  node.name: {node.name}\")\n",
    "    print(f\"  inputs from: {in_names}\")\n",
    "    print(f\"  outputs to : {out_names}\")\n",
    "\n",
    "test_input = torch.randn(1, 64, 32, 32, device=device)\n",
    "original = resnet(test_input)\n",
    "traced = trace_resnet(test_input)\n",
    "assert torch.allclose(original, traced), \"Outputs differ!\"\n",
    "print(\"\\n✓ Traced ResNet block works correctly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finding Conv2d Operations ===\n",
      "\n",
      "Conv2d node: conv1\n",
      "  Module: conv1\n",
      "  Input from: x\n",
      "  Used by: ['relu']\n",
      "\n",
      "Conv2d node: conv2\n",
      "  Module: conv2\n",
      "  Input from: relu\n",
      "  Used by: ['add']\n",
      "\n",
      "Total Conv2d nodes: 2\n",
      "\n",
      "✓ Traced ResNet block works correctly\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = out + identity  # Skip connection\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "resnet_block = ResNetBlock(64).to(device)\n",
    "traced_resnet = fx.symbolic_trace(resnet_block)\n",
    "\n",
    "print(\"=== Finding Conv2d Operations ===\")\n",
    "conv_nodes = []\n",
    "for node in traced_resnet.graph.nodes:\n",
    "    # Conv2d appears as call_module with nn.Conv2d target\n",
    "    if node.op == 'call_module' and isinstance(traced_resnet.get_submodule(node.target), nn.Conv2d):\n",
    "        conv_nodes.append(node)\n",
    "        print(f\"\\nConv2d node: {node.name}\")\n",
    "        print(f\"  Module: {node.target}\")\n",
    "        print(f\"  Input from: {node.args[0].name if node.args else 'None'}\")\n",
    "        print(f\"  Used by: {[user.name for user in node.users.keys()]}\")\n",
    "\n",
    "print(f\"\\nTotal Conv2d nodes: {len(conv_nodes)}\")\n",
    "assert len(conv_nodes) == 2, \"Should have 2 Conv2d nodes\"\n",
    "\n",
    "# Verify traced model works\n",
    "test_input = torch.randn(1, 64, 32, 32, device=device)\n",
    "original = resnet_block(test_input)\n",
    "traced = traced_resnet(test_input)\n",
    "assert torch.allclose(original, traced), \"Outputs differ!\"\n",
    "print(\"\\n✓ Traced ResNet block works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. BASIC GRAPH MANIPULATION (5 min)\n",
    "\n",
    "### Core Operations: Add, Remove, Replace Nodes\n",
    "\n",
    "Graph manipulation requires:\n",
    "1. **Adding nodes**: Use `graph.call_function()`, `graph.call_module()`, etc.\n",
    "2. **Removing nodes**: Use `graph.erase_node()` after removing all users\n",
    "3. **Replacing nodes**: Use `node.replace_all_uses_with()` then erase old node\n",
    "4. **Finalize**: Call `graph.lint()` to validate and `traced.recompile()` to update code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Adding a Node\n",
    "\n",
    "Let's insert a print statement in the middle of our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%fc1,), kwargs = {})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%relu,), kwargs = {})\n",
      "    return fc2\n",
      "\n",
      "=== Modified Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%fc1,), kwargs = {})\n",
      "    %clone : [num_users=1] = call_method[target=clone](args = (%relu,), kwargs = {})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%clone,), kwargs = {})\n",
      "    return fc2\n",
      "\n",
      "✓ Modified model output shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh traced model\n",
    "mlp_for_add = SimpleMLP().to(device)\n",
    "traced_add = fx.symbolic_trace(mlp_for_add)\n",
    "\n",
    "print(\"=== Original Graph ===\")\n",
    "print(traced_add.graph)\n",
    "\n",
    "# Find the relu node and insert a clone operation after it\n",
    "for node in traced_add.graph.nodes:\n",
    "    if node.op == 'call_function' and node.target == torch.relu:\n",
    "        with traced_add.graph.inserting_after(node):\n",
    "            # Add a clone operation (identity operation for demonstration)\n",
    "            new_node = traced_add.graph.call_method('clone', args=(node,))\n",
    "            # Replace all uses of relu with the clone\n",
    "            node.replace_all_uses_with(new_node)\n",
    "            # But clone should use relu as input\n",
    "            new_node.args = (node,)\n",
    "        break\n",
    "\n",
    "traced_add.graph.lint()  # Validate graph\n",
    "traced_add.recompile()   # Regenerate Python code\n",
    "\n",
    "print(\"\\n=== Modified Graph ===\")\n",
    "print(traced_add.graph)\n",
    "\n",
    "# Verify it still works\n",
    "test_input = torch.randn(1, 10, device=device)\n",
    "output = traced_add(test_input)\n",
    "print(f\"\\n✓ Modified model output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Removing a Node\n",
    "\n",
    "Let's remove the ReLU activation from a model. This requires replacing it with an identity operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%fc1,), kwargs = {})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%relu,), kwargs = {})\n",
      "    return fc2\n",
      "\n",
      "=== Graph After ReLU Removal ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%fc1,), kwargs = {})\n",
      "    return fc2\n",
      "\n",
      "✓ Modified model output shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh traced model\n",
    "mlp_for_remove = SimpleMLP().to(device)\n",
    "traced_remove = fx.symbolic_trace(mlp_for_remove)\n",
    "\n",
    "print(\"=== Original Graph ===\")\n",
    "print(traced_remove.graph)\n",
    "\n",
    "# Find and remove relu\n",
    "for node in traced_remove.graph.nodes:\n",
    "    if node.op == 'call_function' and node.target == torch.relu:\n",
    "        # Replace relu with its input (bypass it)\n",
    "        node.replace_all_uses_with(node.args[0])\n",
    "        traced_remove.graph.erase_node(node)\n",
    "        break\n",
    "\n",
    "traced_remove.graph.lint()\n",
    "traced_remove.recompile()\n",
    "\n",
    "print(\"\\n=== Graph After ReLU Removal ===\")\n",
    "print(traced_remove.graph)\n",
    "\n",
    "# Verify - output will differ since we removed activation\n",
    "test_input = torch.randn(1, 10, device=device)\n",
    "output = traced_remove(test_input)\n",
    "print(f\"\\n✓ Modified model output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Replace All ReLU with GELU\n",
    "\n",
    "**Task**: Take the SimpleCNN model, trace it, and replace all ReLU activations with GELU. Verify the transformation works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%conv1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.relu](args = (%conv2,), kwargs = {})\n",
      "    return relu_1\n",
      "\n",
      "Found 2 ReLU nodes to replace\n",
      "\n",
      "=== Modified Graph ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %gelu : [num_users=1] = call_function[target=torch._C._nn.gelu](args = (%conv1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%gelu,), kwargs = {})\n",
      "    %gelu_1 : [num_users=1] = call_function[target=torch._C._nn.gelu](args = (%conv2,), kwargs = {})\n",
      "    return gelu_1\n",
      "\n",
      "=== Generated Code ===\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    gelu = torch._C._nn.gelu(conv1);  conv1 = None\n",
      "    conv2 = self.conv2(gelu);  gelu = None\n",
      "    gelu_1 = torch._C._nn.gelu(conv2);  conv2 = None\n",
      "    return gelu_1\n",
      "    \n",
      "\n",
      "✓ Modified model works! Output shape: torch.Size([1, 32, 32, 32])\n",
      "✓ Successfully replaced 2 ReLU nodes with GELU\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "cnn_for_replace = SimpleCNN().to(device)\n",
    "traced_replace = fx.symbolic_trace(cnn_for_replace)\n",
    "\n",
    "print(\"=== Original Graph ===\")\n",
    "print(traced_replace.graph)\n",
    "\n",
    "# Find all ReLU nodes and replace with GELU\n",
    "nodes_to_replace = []\n",
    "for node in traced_replace.graph.nodes:\n",
    "    if node.op == 'call_function' and node.target == torch.relu:\n",
    "        nodes_to_replace.append(node)\n",
    "\n",
    "print(f\"\\nFound {len(nodes_to_replace)} ReLU nodes to replace\")\n",
    "\n",
    "for relu_node in nodes_to_replace:\n",
    "    with traced_replace.graph.inserting_after(relu_node):\n",
    "        # Create GELU node with same input as ReLU\n",
    "        gelu_node = traced_replace.graph.call_function(\n",
    "            torch.nn.functional.gelu,\n",
    "            args=relu_node.args\n",
    "        )\n",
    "        # Replace all uses of ReLU with GELU\n",
    "        relu_node.replace_all_uses_with(gelu_node)\n",
    "    \n",
    "    # Erase the old ReLU node\n",
    "    traced_replace.graph.erase_node(relu_node)\n",
    "\n",
    "traced_replace.graph.lint()\n",
    "traced_replace.recompile()\n",
    "\n",
    "print(\"\\n=== Modified Graph ===\")\n",
    "print(traced_replace.graph)\n",
    "\n",
    "print(\"\\n=== Generated Code ===\")\n",
    "print(traced_replace.code)\n",
    "\n",
    "# Verify ReLU is gone and GELU is present\n",
    "has_relu = any(node.target == torch.relu for node in traced_replace.graph.nodes if node.op == 'call_function')\n",
    "has_gelu = any(node.target == torch.nn.functional.gelu for node in traced_replace.graph.nodes if node.op == 'call_function')\n",
    "\n",
    "assert not has_relu, \"ReLU still present in graph!\"\n",
    "assert has_gelu, \"GELU not found in graph!\"\n",
    "\n",
    "# Test execution\n",
    "test_input = torch.randn(1, 3, 32, 32, device=device)\n",
    "output = traced_replace(test_input)\n",
    "print(f\"\\n✓ Modified model works! Output shape: {output.shape}\")\n",
    "print(f\"✓ Successfully replaced {len(nodes_to_replace)} ReLU nodes with GELU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You now know:\n",
    "\n",
    "1. **Computation Graphs**: DAGs where nodes are operations and edges are tensors\n",
    "2. **torch.fx**: Symbolic tracing captures model operations into inspectable/modifiable graphs\n",
    "3. **Graph Inspection**: Iterate nodes, check types (placeholder, call_module, call_function, output), analyze connections\n",
    "4. **Graph Manipulation**: Add nodes with `graph.call_*()`, remove with `erase_node()`, replace with `replace_all_uses_with()`\n",
    "\n",
    "**Next Steps for Building a Graph Optimizer**:\n",
    "- Learn pattern matching: `fx.passes.graph_matcher`\n",
    "- Study fusion patterns: combine ops like conv+bn+relu into single optimized kernels\n",
    "- Explore compiler passes: dead code elimination, constant folding\n",
    "- Understand memory optimization: inplace operations, buffer reuse\n",
    "\n",
    "**Key Workflow**:\n",
    "1. Trace model with `fx.symbolic_trace()`\n",
    "2. Iterate and analyze with `graph.nodes`\n",
    "3. Modify graph (add/remove/replace nodes)\n",
    "4. Validate with `graph.lint()`\n",
    "5. Recompile with `module.recompile()`\n",
    "6. Test modified model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference Card\n",
    "\n",
    "```python\n",
    "# Trace a model\n",
    "traced = fx.symbolic_trace(model)\n",
    "\n",
    "# Iterate nodes\n",
    "for node in traced.graph.nodes:\n",
    "    print(node.op, node.target, node.args)\n",
    "\n",
    "# Node types\n",
    "node.op in ['placeholder', 'call_module', 'call_function', 'call_method', 'output']\n",
    "\n",
    "# Add node after current node\n",
    "with graph.inserting_after(node):\n",
    "    new = graph.call_function(torch.relu, args=(node,))\n",
    "\n",
    "# Replace node\n",
    "old_node.replace_all_uses_with(new_node)\n",
    "graph.erase_node(old_node)\n",
    "\n",
    "# Finalize changes\n",
    "graph.lint()\n",
    "traced.recompile()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
