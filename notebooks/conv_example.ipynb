{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2d Crash Course for Graph Optimization (10 minutes)\n",
    "\n",
    "Learn exactly what you need to understand Conv2d operations for graph optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. WHAT IS A CONVOLUTION? (2D Image Example)\n",
    "\n",
    "A convolution slides a small filter (kernel) over an image to detect patterns.\n",
    "\n",
    "```\n",
    "Example: 3x3 kernel sliding over a 5x5 image:\n",
    "\n",
    "   Input Image (5x5)              3x3 Kernel           Output Feature Map\n",
    "\n",
    "   [1 2 3 4 5]                    [1 0 1]\n",
    "   [6 7 8 9 0]                    [0 1 0]              Each position =\n",
    "   [1 2 3 4 5]        *           [1 0 1]       →      sum of element-wise\n",
    "   [6 7 8 9 0]                                         multiplication\n",
    "   [1 2 3 4 5]\n",
    "```\n",
    "\n",
    "**The kernel slides across the image:**\n",
    "- Position 1: Kernel over top-left 3x3 → compute one output value\n",
    "- Position 2: Slide right by stride → compute next output value\n",
    "- Continue until entire image is covered\n",
    "\n",
    "**Key concepts:**\n",
    "- Each output value = weighted sum of local region (kernel acts as pattern detector)\n",
    "- Multiple kernels = multiple output channels (detect different patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. KEY PARAMETERS\n",
    "\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `in_channels`: Number of input feature maps (e.g., 3 for RGB image)\n",
    "- `out_channels`: Number of output feature maps (number of different kernels/filters)\n",
    "- `kernel_size`: Size of the sliding window (e.g., 3 means 3x3 kernel)\n",
    "- `stride`: How many pixels to move kernel each step (default=1)\n",
    "- `padding`: Pixels added around border (padding=1 adds 1 pixel border of zeros)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "```\n",
    "- Takes 3-channel input (RGB)\n",
    "- Applies 16 different 3x3 kernels\n",
    "- Produces 16 output feature maps\n",
    "- padding=1 keeps spatial dimensions the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SHAPE CALCULATION FORMULAS ⭐\n",
    "\n",
    "**Input shape:** `(batch, in_channels, H_in, W_in)`  \n",
    "**Output shape:** `(batch, out_channels, H_out, W_out)`\n",
    "\n",
    "### Height/Width Calculation:\n",
    "```\n",
    "H_out = floor((H_in + 2*padding - kernel_size) / stride) + 1\n",
    "W_out = floor((W_in + 2*padding - kernel_size) / stride) + 1\n",
    "```\n",
    "\n",
    "### Common Cases:\n",
    "\n",
    "**1. Keep same size:** `padding = (kernel_size - 1) / 2`\n",
    "   - Example: kernel=3, padding=1 → same size\n",
    "\n",
    "**2. Halve size:** `stride=2, padding=1, kernel=3`\n",
    "   - Example: 32x32 → 16x16\n",
    "\n",
    "**3. No padding:** `padding=0`\n",
    "   - Example: 32x32 with kernel=3 → 30x30\n",
    "\n",
    "### Other Layers:\n",
    "\n",
    "**BatchNorm2d(num_features):**\n",
    "- `num_features` must equal the number of channels\n",
    "- Does NOT change shape: `(B, C, H, W) → (B, C, H, W)`\n",
    "\n",
    "**MaxPool2d(kernel_size, stride):**\n",
    "- Typically kernel_size = stride (non-overlapping)\n",
    "- Output shape: `H_out = H_in / stride`, `W_out = W_in / stride`\n",
    "- Example: MaxPool2d(2, 2) halves both dimensions\n",
    "\n",
    "**ReLU():**\n",
    "- Does NOT change shape: `(B, C, H, W) → (B, C, H, W)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. STEP-BY-STEP SHAPE TRACKING EXAMPLE ⭐⭐\n",
    "\n",
    "Let's track shapes through a typical CNN block:\n",
    "\n",
    "```\n",
    "Starting input: (1, 3, 32, 32)\n",
    "                 ↓ batch=1, channels=3 (RGB), 32x32 pixels\n",
    "\n",
    "Layer 1: Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "  H_out = (32 + 2*1 - 3)/1 + 1 = 32\n",
    "  W_out = (32 + 2*1 - 3)/1 + 1 = 32\n",
    "  Shape: (1, 16, 32, 32)  ← 3 channels → 16 channels, size preserved\n",
    "                           ↓\n",
    "\n",
    "Layer 2: BatchNorm2d(16)\n",
    "  Shape: (1, 16, 32, 32)  ← No change (normalizes per channel)\n",
    "                           ↓\n",
    "\n",
    "Layer 3: ReLU()\n",
    "  Shape: (1, 16, 32, 32)  ← No change (elementwise activation)\n",
    "                           ↓\n",
    "\n",
    "Layer 4: MaxPool2d(2, 2)\n",
    "  H_out = 32/2 = 16\n",
    "  W_out = 32/2 = 16\n",
    "  Shape: (1, 16, 16, 16)  ← Spatial dimensions halved\n",
    "                           ↓\n",
    "\n",
    "Final output: (1, 16, 16, 16)\n",
    "```\n",
    "\n",
    "### KEY INSIGHT FOR STACKING LAYERS:\n",
    "\n",
    "**Next layer's in_channels MUST equal previous layer's out_channels!**\n",
    "\n",
    "```python\n",
    "✓ Conv2d(3, 16, ...) → Conv2d(16, 32, ...)  # 16 matches!\n",
    "✗ Conv2d(3, 16, ...) → Conv2d(8, 32, ...)   # ERROR: 16 ≠ 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. WORKING CODE EXAMPLE\n",
    "\n",
    "Let's build a 2-block CNN and track shapes through each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running forward pass:\n",
      "\n",
      "Input:               torch.Size([1, 3, 32, 32])\n",
      "After Conv2d(3→16):  torch.Size([1, 16, 32, 32])\n",
      "After BatchNorm:     torch.Size([1, 16, 32, 32])\n",
      "After ReLU:          torch.Size([1, 16, 32, 32])\n",
      "After MaxPool(2x2):  torch.Size([1, 16, 16, 16])\n",
      "After Conv2d(16→32): torch.Size([1, 32, 16, 16])\n",
      "After BatchNorm:     torch.Size([1, 32, 16, 16])\n",
      "After ReLU:          torch.Size([1, 32, 16, 16])\n",
      "After MaxPool(2x2):  torch.Size([1, 32, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "class DetailedConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Block 1: 3 → 16 channels\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)  # Must match conv1 out_channels\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Block 2: 16 → 32 channels (in_channels must match previous out_channels!)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)  # Must match conv2 out_channels\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input:               {x.shape}\")\n",
    "\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        print(f\"After Conv2d(3→16):  {x.shape}\")\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        print(f\"After BatchNorm:     {x.shape}\")\n",
    "\n",
    "        x = torch.relu(x)\n",
    "        print(f\"After ReLU:          {x.shape}\")\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        print(f\"After MaxPool(2x2):  {x.shape}\")\n",
    "\n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        print(f\"After Conv2d(16→32): {x.shape}\")\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        print(f\"After BatchNorm:     {x.shape}\")\n",
    "\n",
    "        x = torch.relu(x)\n",
    "        print(f\"After ReLU:          {x.shape}\")\n",
    "\n",
    "        x = self.pool2(x)\n",
    "        print(f\"After MaxPool(2x2):  {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "# Test the network\n",
    "model = DetailedConvNet().to(device)\n",
    "test_input = torch.randn(1, 3, 32, 32, device=device)\n",
    "\n",
    "print(\"Running forward pass:\\n\")\n",
    "output = model(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Shape Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected shape progression:\n",
      "  (1, 3, 32, 32)   Input\n",
      "→ (1, 16, 32, 32)  Conv2d: channels 3→16, size preserved by padding=1\n",
      "→ (1, 16, 32, 32)  BatchNorm: no change\n",
      "→ (1, 16, 32, 32)  ReLU: no change\n",
      "→ (1, 16, 16, 16)  MaxPool: size halved (32→16)\n",
      "→ (1, 32, 16, 16)  Conv2d: channels 16→32, size preserved by padding=1\n",
      "→ (1, 32, 16, 16)  BatchNorm: no change\n",
      "→ (1, 32, 16, 16)  ReLU: no change\n",
      "→ (1, 32, 8, 8)    MaxPool: size halved (16→8)\n",
      "\n",
      "Actual final shape: torch.Size([1, 32, 8, 8])\n",
      "\n",
      "✓ All shape calculations correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected shape progression:\")\n",
    "print(\"  (1, 3, 32, 32)   Input\")\n",
    "print(\"→ (1, 16, 32, 32)  Conv2d: channels 3→16, size preserved by padding=1\")\n",
    "print(\"→ (1, 16, 32, 32)  BatchNorm: no change\")\n",
    "print(\"→ (1, 16, 32, 32)  ReLU: no change\")\n",
    "print(\"→ (1, 16, 16, 16)  MaxPool: size halved (32→16)\")\n",
    "print(\"→ (1, 32, 16, 16)  Conv2d: channels 16→32, size preserved by padding=1\")\n",
    "print(\"→ (1, 32, 16, 16)  BatchNorm: no change\")\n",
    "print(\"→ (1, 32, 16, 16)  ReLU: no change\")\n",
    "print(\"→ (1, 32, 8, 8)    MaxPool: size halved (16→8)\")\n",
    "print(f\"\\nActual final shape: {output.shape}\")\n",
    "\n",
    "assert output.shape == torch.Size([1, 32, 8, 8]), \"Shape mismatch!\"\n",
    "print(\"\\n✓ All shape calculations correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUICK REFERENCE FOR GRAPH OPTIMIZATION\n",
    "\n",
    "When analyzing torch.fx graphs, you'll see:\n",
    "\n",
    "### 1. Module Nodes\n",
    "```python\n",
    "# call_module nodes with target like \"conv1\", \"bn1\", \"pool1\"\n",
    "# Use get_submodule(target) to get the actual module\n",
    "# Check isinstance(module, nn.Conv2d) to identify conv layers\n",
    "```\n",
    "\n",
    "### 2. Channel Tracking\n",
    "```python\n",
    "# Conv2d has .in_channels and .out_channels attributes\n",
    "# BatchNorm2d has .num_features (must match conv out_channels)\n",
    "# These must align when stacking layers!\n",
    "```\n",
    "\n",
    "### 3. Spatial Size Tracking\n",
    "```python\n",
    "# Conv2d: use formula with padding, kernel_size, stride\n",
    "# MaxPool2d: typically divides by stride\n",
    "# ReLU/BatchNorm: preserve spatial dimensions\n",
    "```\n",
    "\n",
    "### 4. Common Optimizations\n",
    "```python\n",
    "# Fuse Conv2d + BatchNorm2d into single conv (inference only)\n",
    "# Fuse Conv2d + ReLU into single operation\n",
    "# Replace sequence of ops with optimized kernel\n",
    "# These require matching input/output shapes!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOU'RE READY!\n",
    "\n",
    "You now understand Conv2d well enough to:\n",
    "- ✓ Read torch.fx graphs with conv operations\n",
    "- ✓ Track tensor shapes through transformations\n",
    "- ✓ Identify which layers can be fused/optimized\n",
    "- ✓ Verify shape compatibility when modifying graphs\n",
    "\n",
    "**Go tackle Exercise 1 in `00_essentials_only.ipynb`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice: Calculate Shapes Manually\n",
    "\n",
    "Given this architecture, calculate the output shape at each step:\n",
    "\n",
    "```python\n",
    "Input: (2, 3, 64, 64)  # batch=2, RGB, 64x64 image\n",
    "Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "ReLU()\n",
    "MaxPool2d(2, 2)\n",
    "Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "ReLU()\n",
    "MaxPool2d(2, 2)\n",
    "```\n",
    "\n",
    "Try calculating before running the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:          torch.Size([2, 3, 64, 64])\n",
      "After Conv1:    torch.Size([2, 32, 64, 64])\n",
      "After ReLU:     torch.Size([2, 32, 64, 64])\n",
      "After Pool1:    torch.Size([2, 32, 32, 32])\n",
      "After Conv2:    torch.Size([2, 64, 32, 32])\n",
      "After ReLU:     torch.Size([2, 64, 32, 32])\n",
      "After Pool2:    torch.Size([2, 64, 16, 16])\n",
      "\n",
      "✓ Final shape: torch.Size([2, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Solution:\n",
    "class PracticeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input:          {x.shape}\")  # (2, 3, 64, 64)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        # H_out = (64 + 2*2 - 5)/1 + 1 = 64\n",
    "        print(f\"After Conv1:    {x.shape}\")  # (2, 32, 64, 64)\n",
    "        \n",
    "        x = torch.relu(x)\n",
    "        print(f\"After ReLU:     {x.shape}\")  # (2, 32, 64, 64)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        # H_out = 64/2 = 32\n",
    "        print(f\"After Pool1:    {x.shape}\")  # (2, 32, 32, 32)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        # H_out = (32 + 2*1 - 3)/1 + 1 = 32\n",
    "        print(f\"After Conv2:    {x.shape}\")  # (2, 64, 32, 32)\n",
    "        \n",
    "        x = torch.relu(x)\n",
    "        print(f\"After ReLU:     {x.shape}\")  # (2, 64, 32, 32)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        # H_out = 32/2 = 16\n",
    "        print(f\"After Pool2:    {x.shape}\")  # (2, 64, 16, 16)\n",
    "        \n",
    "        return x\n",
    "\n",
    "practice_model = PracticeNet().to(device)\n",
    "practice_input = torch.randn(2, 3, 64, 64, device=device)\n",
    "practice_output = practice_model(practice_input)\n",
    "\n",
    "print(f\"\\n✓ Final shape: {practice_output.shape}\")\n",
    "assert practice_output.shape == torch.Size([2, 64, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
